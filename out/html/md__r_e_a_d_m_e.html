<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ORB_SLAM3: ORB-SLAM3</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ORB_SLAM3
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">ORB-SLAM3 </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md18"></a>
V1.0, December 22th, 2021</h2>
<p><b>Authors:</b> Carlos Campos, Richard Elvira, Juan J. Gómez Rodríguez, <a href="http://webdiis.unizar.es/~josemari/">José M. M. Montiel</a>, <a href="http://webdiis.unizar.es/~jdtardos/">Juan D. Tardos</a>.</p>
<p>The <a href="https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Changelog.md">Changelog</a> describes the features of each version.</p>
<p>ORB-SLAM3 is the first real-time SLAM library able to perform <b>Visual, Visual-Inertial and Multi-Map SLAM</b> with <b>monocular, stereo and RGB-D</b> cameras, using <b>pin-hole and fisheye</b> lens models. In all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature, and significantly more accurate.</p>
<p>We provide examples to run ORB-SLAM3 in the <a href="http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets">EuRoC dataset</a> using stereo or monocular, with or without IMU, and in the <a href="https://vision.in.tum.de/data/datasets/visual-inertial-dataset">TUM-VI dataset</a> using fisheye stereo or monocular, with or without IMU. Videos of some example executions can be found at <a href="https://www.youtube.com/channel/UCXVt-kXG6T95Z4tVaYlU80Q">ORB-SLAM3 channel</a>.</p>
<p>This software is based on <a href="https://github.com/raulmur/ORB_SLAM2">ORB-SLAM2</a> developed by <a href="http://webdiis.unizar.es/~raulmur/">Raul Mur-Artal</a>, <a href="http://webdiis.unizar.es/~jdtardos/">Juan D. Tardos</a>, <a href="http://webdiis.unizar.es/~josemari/">J. M. M. Montiel</a> and <a href="http://doriangalvez.com/">Dorian Galvez-Lopez</a> (<a href="https://github.com/dorian3d/DBoW2">DBoW2</a>).</p>
<p><a href="https://youtu.be/HyLNq-98LRo" target="_blank"><img src="https://img.youtube.com/vi/HyLNq-98LRo/0.jpg" alt="ORB-SLAM3" width="240" height="180" border="10" class="inline"/></a></p>
<h2><a class="anchor" id="autotoc_md19"></a>
Related Publications:</h2>
<p>[ORB-SLAM3] Carlos Campos, Richard Elvira, Juan J. Gómez Rodríguez, José M. M. Montiel and Juan D. Tardós, <b>ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM</b>, <em>IEEE Transactions on Robotics 37(6):1874-1890, Dec. 2021</em>. <b><a href="https://arxiv.org/abs/2007.11898">PDF</a></b>.</p>
<p>[IMU-Initialization] Carlos Campos, J. M. M. Montiel and Juan D. Tardós, <b>Inertial-Only Optimization for Visual-Inertial Initialization</b>, <em>ICRA 2020</em>. <b><a href="https://arxiv.org/pdf/2003.05766.pdf">PDF</a></b></p>
<p>[ORBSLAM-Atlas] Richard Elvira, J. M. M. Montiel and Juan D. Tardós, <b>ORBSLAM-Atlas: a robust and accurate multi-map system</b>, <em>IROS 2019</em>. <b><a href="https://arxiv.org/pdf/1908.11585.pdf">PDF</a></b>.</p>
<p>[ORBSLAM-VI] Raúl Mur-Artal, and Juan D. Tardós, <b>Visual-inertial monocular SLAM with map reuse</b>, IEEE Robotics and Automation Letters, vol. 2 no. 2, pp. 796-803, 2017. <b><a href="https://arxiv.org/pdf/1610.05949.pdf">PDF</a></b>.</p>
<p>[Stereo and RGB-D] Raúl Mur-Artal and Juan D. Tardós. <b>ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras</b>. <em>IEEE Transactions on Robotics,</em> vol. 33, no. 5, pp. 1255-1262, 2017. <b><a href="https://arxiv.org/pdf/1610.06475.pdf">PDF</a></b>.</p>
<p>[Monocular] Raúl Mur-Artal, José M. M. Montiel and Juan D. Tardós. <b>ORB-SLAM: A Versatile and Accurate Monocular SLAM System</b>. <em>IEEE Transactions on Robotics,</em> vol. 31, no. 5, pp. 1147-1163, 2015. (<b>2015 IEEE Transactions on Robotics Best Paper Award</b>). <b><a href="https://arxiv.org/pdf/1502.00956.pdf">PDF</a></b>.</p>
<p>[<a class="el" href="namespace_d_bo_w2.html">DBoW2</a> Place Recognition] Dorian Gálvez-López and Juan D. Tardós. <b>Bags of Binary Words for Fast Place Recognition in Image Sequences</b>. <em>IEEE Transactions on Robotics,</em> vol. 28, no. 5, pp. 1188-1197, 2012. <b><a href="http://doriangalvez.com/php/dl.php?dlp=GalvezTRO12.pdf">PDF</a></b></p>
<h1><a class="anchor" id="autotoc_md20"></a>
1. License</h1>
<p>ORB-SLAM3 is released under <a href="https://github.com/UZ-SLAMLab/ORB_SLAM3/LICENSE">GPLv3 license</a>. For a list of all code/library dependencies (and associated licenses), please see <a href="https://github.com/UZ-SLAMLab/ORB_SLAM3/blob/master/Dependencies.md">Dependencies.md</a>.</p>
<p>For a closed-source version of ORB-SLAM3 for commercial purposes, please contact the authors: orbslam (at) unizar (dot) es.</p>
<p>If you use ORB-SLAM3 in an academic work, please cite: </p><pre class="fragment">@article{ORBSLAM3_TRO,
  title={{ORB-SLAM3}: An Accurate Open-Source Library for Visual, Visual-Inertial 
           and Multi-Map {SLAM}},
  author={Campos, Carlos AND Elvira, Richard AND G\´omez, Juan J. AND Montiel, 
          Jos\'e M. M. AND Tard\'os, Juan D.},
  journal={IEEE Transactions on Robotics}, 
  volume={37},
  number={6},
  pages={1874-1890},
  year={2021}
 }
</pre><h1><a class="anchor" id="autotoc_md21"></a>
2. Prerequisites</h1>
<p>We have tested the library in <b>Ubuntu 16.04</b> and <b>18.04</b>, but it should be easy to compile in other platforms. A powerful computer (e.g. i7) will ensure real-time performance and provide more stable and accurate results.</p>
<h2><a class="anchor" id="autotoc_md22"></a>
C++11 or C++0x Compiler</h2>
<p>We use the new thread and chrono functionalities of C++11.</p>
<h2><a class="anchor" id="autotoc_md23"></a>
Pangolin</h2>
<p>We use <a href="https://github.com/stevenlovegrove/Pangolin">Pangolin</a> for visualization and user interface. Dowload and install instructions can be found at: <a href="https://github.com/stevenlovegrove/Pangolin">https://github.com/stevenlovegrove/Pangolin</a>.</p>
<h2><a class="anchor" id="autotoc_md24"></a>
OpenCV</h2>
<p>We use <a href="http://opencv.org">OpenCV</a> to manipulate images and features. Dowload and install instructions can be found at: <a href="http://opencv.org">http://opencv.org</a>. <b>Required at leat 3.0. Tested with OpenCV 3.2.0 and 4.4.0</b>.</p>
<h2><a class="anchor" id="autotoc_md25"></a>
Eigen3</h2>
<p>Required by <a class="el" href="namespaceg2o.html">g2o</a> (see below). Download and install instructions can be found at: <a href="http://eigen.tuxfamily.org">http://eigen.tuxfamily.org</a>. <b>Required at least 3.1.0</b>.</p>
<h2><a class="anchor" id="autotoc_md26"></a>
PCL (added by Awei)</h2>
<p>Required for saving the map in PCD format. I run the modified code with pcl-1.8 in Ubuntu 18.04. Here I just use the basic functions in PCL, so I think older versions of PCL also work.</p>
<h2><a class="anchor" id="autotoc_md27"></a>
DBoW2 and g2o (Included in Thirdparty folder)</h2>
<p>We use modified versions of the <a href="https://github.com/dorian3d/DBoW2">DBoW2</a> library to perform place recognition and <a href="https://github.com/RainerKuemmerle/g2o">g2o</a> library to perform non-linear optimizations. Both modified libraries (which are BSD) are included in the <em>Thirdparty</em> folder.</p>
<h2><a class="anchor" id="autotoc_md28"></a>
Python</h2>
<p>Required to calculate the alignment of the trajectory with the ground truth. <b>Required Numpy module</b>.</p>
<ul>
<li>(win) <a href="http://www.python.org/downloads/windows">http://www.python.org/downloads/windows</a></li>
<li>(deb) <code>sudo apt install libpython2.7-dev</code></li>
<li>(mac) preinstalled with osx</li>
</ul>
<h2><a class="anchor" id="autotoc_md29"></a>
ROS (optional)</h2>
<p>We provide some examples to process input of a monocular, monocular-inertial, stereo, stereo-inertial or RGB-D camera using ROS. Building these examples is optional. These have been tested with ROS Melodic under Ubuntu 18.04.</p>
<h1><a class="anchor" id="autotoc_md30"></a>
3. Building ORB-SLAM3 library and examples</h1>
<p>Clone the repository: </p><div class="fragment"><div class="line">git clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git ORB_SLAM3</div>
</div><!-- fragment --><p>We provide a script <code>build.sh</code> to build the <em>Thirdparty</em> libraries and <em>ORB-SLAM3</em>. Please make sure you have installed all required dependencies (see section 2). Execute: </p><div class="fragment"><div class="line">cd ORB_SLAM3</div>
<div class="line">chmod +x build.sh</div>
<div class="line">./build.sh</div>
</div><!-- fragment --><p>This will create <b>libORB_SLAM3.so</b> at <em>lib</em> folder and the executables in <em>Examples</em> folder.</p>
<h1><a class="anchor" id="autotoc_md31"></a>
4. Running ORB-SLAM3 with your camera</h1>
<p>Directory <code>Examples</code> contains several demo programs and calibration files to run ORB-SLAM3 in all sensor configurations with Intel Realsense cameras T265 and D435i. The steps needed to use your own camera are:</p>
<ol type="1">
<li>Calibrate your camera following <code>Calibration_Tutorial.pdf</code> and write your calibration file <code>your_camera.yaml</code></li>
<li>Modify one of the provided demos to suit your specific camera model, and build it</li>
<li>Connect the camera to your computer using USB3 or the appropriate interface</li>
<li>Run ORB-SLAM3. For example, for our D435i camera, we would execute:</li>
</ol>
<div class="fragment"><div class="line">./Examples/Stereo-Inertial/stereo_inertial_realsense_D435i Vocabulary/ORBvoc.txt ./Examples/Stereo-Inertial/RealSense_D435i.yaml</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md32"></a>
5. EuRoC Examples</h1>
<p><a href="http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets">EuRoC dataset</a> was recorded with two pinhole cameras and an inertial sensor. We provide an example script to launch EuRoC sequences in all the sensor configurations.</p>
<ol type="1">
<li>Download a sequence (ASL format) from <a href="http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets">http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets</a></li>
<li>Open the script "euroc_examples.sh" in the root of the project. Change <b>pathDatasetEuroc</b> variable to point to the directory where the dataset has been uncompressed.</li>
<li>Execute the following script to process all the sequences with all sensor configurations: <div class="fragment"><div class="line">./euroc_examples</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md33"></a>
Evaluation</h2>
<p>EuRoC provides ground truth for each sequence in the IMU body reference. As pure visual executions report trajectories centered in the left camera, we provide in the "evaluation" folder the transformation of the ground truth to the left camera reference. Visual-inertial trajectories use the ground truth from the dataset.</p>
<p>Execute the following script to process sequences and compute the RMS ATE: </p><div class="fragment"><div class="line">./euroc_eval_examples</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md34"></a>
6. TUM-VI Examples</h1>
<p><a href="https://vision.in.tum.de/data/datasets/visual-inertial-dataset">TUM-VI dataset</a> was recorded with two fisheye cameras and an inertial sensor.</p>
<ol type="1">
<li>Download a sequence from <a href="https://vision.in.tum.de/data/datasets/visual-inertial-dataset">https://vision.in.tum.de/data/datasets/visual-inertial-dataset</a> and uncompress it.</li>
<li>Open the script "tum_vi_examples.sh" in the root of the project. Change <b>pathDatasetTUM_VI</b> variable to point to the directory where the dataset has been uncompressed.</li>
<li>Execute the following script to process all the sequences with all sensor configurations: <div class="fragment"><div class="line">./tum_vi_examples</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="autotoc_md35"></a>
Evaluation</h2>
<p>In TUM-VI ground truth is only available in the room where all sequences start and end. As a result the error measures the drift at the end of the sequence.</p>
<p>Execute the following script to process sequences and compute the RMS ATE: </p><div class="fragment"><div class="line">./tum_vi_eval_examples</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md36"></a>
7. ROS Examples</h1>
<h3><a class="anchor" id="autotoc_md37"></a>
Building the nodes for mono, mono-inertial, stereo, stereo-inertial and RGB-D</h3>
<p>Tested with ROS Melodic and ubuntu 18.04.</p>
<ol type="1">
<li>Add the path including <em>Examples/ROS/ORB_SLAM3</em> to the ROS_PACKAGE_PATH environment variable. Open .bashrc file: <div class="fragment"><div class="line">gedit ~/.bashrc</div>
</div><!-- fragment --></li>
</ol>
<p>and add at the end the following line. Replace PATH by the folder where you cloned <a class="el" href="namespace_o_r_b___s_l_a_m3.html">ORB_SLAM3</a>:</p>
<div class="fragment"><div class="line">export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:PATH/ORB_SLAM3/Examples/ROS</div>
</div><!-- fragment --><ol type="1">
<li>Execute <code>build_ros.sh</code> script:</li>
</ol>
<div class="fragment"><div class="line">chmod +x build_ros.sh</div>
<div class="line">./build_ros.sh</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md38"></a>
Running Monocular Node</h3>
<p>For a monocular input from topic <code>/camera/image_raw</code> run node ORB_SLAM3/Mono. You will need to provide the vocabulary file and a settings file. See the monocular examples above.</p>
<div class="fragment"><div class="line">rosrun ORB_SLAM3 Mono PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md39"></a>
Running Monocular-Inertial Node</h3>
<p>For a monocular input from topic <code>/camera/image_raw</code> and an inertial input from topic <code>/imu</code>, run node ORB_SLAM3/Mono_Inertial. Setting the optional third argument to true will apply CLAHE equalization to images (Mainly for TUM-VI dataset).</p>
<div class="fragment"><div class="line">rosrun ORB_SLAM3 Mono PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE [EQUALIZATION] </div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md40"></a>
Running Stereo Node</h3>
<p>For a stereo input from topic <code>/camera/left/image_raw</code> and <code>/camera/right/image_raw</code> run node ORB_SLAM3/Stereo. You will need to provide the vocabulary file and a settings file. For Pinhole camera model, if you <b>provide rectification matrices</b> (see Examples/Stereo/EuRoC.yaml example), the node will recitify the images online, <b>otherwise images must be pre-rectified</b>. For FishEye camera model, rectification is not required since system works with original images:</p>
<div class="fragment"><div class="line">rosrun ORB_SLAM3 Stereo PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE ONLINE_RECTIFICATION</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md41"></a>
Running Stereo-Inertial Node</h3>
<p>For a stereo input from topics <code>/camera/left/image_raw</code> and <code>/camera/right/image_raw</code>, and an inertial input from topic <code>/imu</code>, run node ORB_SLAM3/Stereo_Inertial. You will need to provide the vocabulary file and a settings file, including rectification matrices if required in a similar way to Stereo case:</p>
<div class="fragment"><div class="line">rosrun ORB_SLAM3 Stereo_Inertial PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE ONLINE_RECTIFICATION [EQUALIZATION] </div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md42"></a>
Running RGB_D Node</h3>
<p>For an RGB-D input from topics <code>/camera/rgb/image_raw</code> and <code>/camera/depth_registered/image_raw</code>, run node ORB_SLAM3/RGBD. You will need to provide the vocabulary file and a settings file. See the RGB-D example above.</p>
<div class="fragment"><div class="line">rosrun ORB_SLAM3 RGBD PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE</div>
</div><!-- fragment --><p><b>Running ROS example:</b> Download a rosbag (e.g. V1_02_medium.bag) from the EuRoC dataset (<a href="http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets">http://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets</a>). Open 3 tabs on the terminal and run the following command at each tab for a Stereo-Inertial configuration: </p><div class="fragment"><div class="line">roscore</div>
</div><!-- fragment --><div class="fragment"><div class="line">rosrun ORB_SLAM3 Stereo_Inertial Vocabulary/ORBvoc.txt Examples/Stereo-Inertial/EuRoC.yaml true</div>
</div><!-- fragment --><div class="fragment"><div class="line">rosbag play --pause V1_02_medium.bag /cam0/image_raw:=/camera/left/image_raw /cam1/image_raw:=/camera/right/image_raw /imu0:=/imu</div>
</div><!-- fragment --><p>Once ORB-SLAM3 has loaded the vocabulary, press space in the rosbag tab.</p>
<p><b>Remark:</b> For rosbags from TUM-VI dataset, some play issue may appear due to chunk size. One possible solution is to rebag them with the default chunk size, for example: </p><div class="fragment"><div class="line">rosrun rosbag fastrebag.py dataset-room1_512_16.bag dataset-room1_512_16_small_chunks.bag</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md43"></a>
8. Running time analysis</h1>
<p>A flag in <code>include\Config.h</code> activates time measurements. It is necessary to uncomment the line <code>#define REGISTER_TIMES</code> to obtain the time stats of one execution which is shown at the terminal and stored in a text file(<code>ExecTimeMean.txt</code>).</p>
<h1><a class="anchor" id="autotoc_md44"></a>
9. Calibration</h1>
<p>You can find a tutorial for visual-inertial calibration and a detailed description of the contents of valid configuration files at <code>Calibration_Tutorial.pdf</code></p>
<h1><a class="anchor" id="autotoc_md45"></a>
10. Save function (added by Awei)</h1>
<p>I modified FrameDrawer.cc and MapDrawer.cc in src folder, and the modified <a class="el" href="namespace_o_r_b___s_l_a_m3.html">ORB_SLAM3</a> will save a txt file containing the keypoints with frame IDs and a pcd file containing the map. These files are saved in the path where you run the program.</p>
<p>I tested the modified <a class="el" href="namespace_o_r_b___s_l_a_m3.html">ORB_SLAM3</a> with a EuRoC sequence (V1_01_easy) in Stereo mode. The saved txt file is shown below, you can edit a script to further process saved keypoints</p>
<p><img src="https://github.com/DioVei/ORB_SLAM3_with_save/blob/master/example.png" alt="Image text" class="inline"/></p>
<p>and the save pcd file is shown below by pcl_viewer</p>
<p><img src="https://github.com/DioVei/ORB_SLAM3_with_save/blob/master/result.png" alt="Image text" class="inline"/></p>
<h1><a class="anchor" id="autotoc_md46"></a>
Update</h1>
<p>When you run the program in a large scene, you can rename MapDrawer.cc to MapDrawer_SmallScene.cc, and MapDrawer_BigScene.cc to MapDrawer.cc in the src folder. Then, you can rebuild the program to make the saving function adapt to large scenes. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Sat Aug 26 2023 09:59:14 for ORB_SLAM3 by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
